{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in library(modules):\n",
      "\"Packages loaded with 'library' may not be available inside a module. For loading packages in a module, use 'import' instead.\"Warning message in library(rlang):\n",
      "\"Packages loaded with 'library' may not be available inside a module. For loading packages in a module, use 'import' instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 1453 rows."
     ]
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "library(caret)\n",
    "library(car)\n",
    "library(glmnet)\n",
    "library(dplyr)\n",
    "\n",
    "setwd('C:/Users/iceca/Documents/Housing_Regression')\n",
    "\n",
    "set.seed(42)\n",
    "\n",
    "m <- modules::use(\"Helpers/Loading_Data.R\")\n",
    "m <- m$load.train\n",
    "\n",
    "dat <- m$skew.data()\n",
    "\n",
    "test <- dat[[2]] \n",
    "colnames(test) <- make.names(colnames(test))\n",
    "\n",
    "train <- dat[[1]]\n",
    "labels <- m$get.labels() %>% arrange(Id) %>% filter(Id %in% train$Id)\n",
    "train <- dat[[1]] %>% arrange(Id) %>% mutate(SalePrice = labels$SalePrice, NumberLevel= as.numeric(train$NumberLevel)) \n",
    "colnames(train) <- make.names(colnames(train))\n",
    "train <- train %>% mutate_if(is.logical, function(x) {as.numeric(x)})\n",
    "\n",
    "cat(\"train has\" , nrow(train), \"rows.\")\n",
    "#I need to combine the train and test into one model matrix to ensure that train and test have the same factors\n",
    "dat.matrix <- sparse.model.matrix(SalePrice ~. , rbind( train , test %>% mutate(SalePrice=0) ) )\n",
    "train.matrix <- dat.matrix[1:nrow(train),]\n",
    "test.matrix <- dat.matrix[ (1+nrow(train)) : dim(dat.matrix)[1] ,]\n",
    "\n",
    "cross.validation.index <- createDataPartition(labels$SalePrice,times = 1,p = 0.15,list = FALSE)\n",
    "cv <- train[cross.validation.index , ]\n",
    "train.cv <- train[-cross.validation.index , ]\n",
    "train.raw <- m$featEngRaw.data()[[1]]\n",
    "cross.validation.index <- cross.validation.index %>% as.vector()\n",
    "cv.matrix <- train.matrix[cross.validation.index, ]\n",
    "cv.matrix.labels <- labels$SalePrice[cross.validation.index]\n",
    "train.cv.matrix <- train.matrix[-cross.validation.index, ]\n",
    "train.cv.matrix.labels <- labels$SalePrice[-cross.validation.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>148</li>\n",
       "\t<li>216</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 148\n",
       "\\item 216\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 148\n",
       "2. 216\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 148 216"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>parameter</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>none</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       " parameter\\\\\n",
       "\\hline\n",
       "\t none\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| parameter |\n",
       "|---|\n",
       "| none |\n",
       "\n"
      ],
      "text/plain": [
       "  parameter\n",
       "1 none     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>parameter</th><th scope=col>logMSE</th><th scope=col>logMSESD</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>none      </td><td>0.1890392 </td><td>0.01240303</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " parameter & logMSE & logMSESD\\\\\n",
       "\\hline\n",
       "\t none       & 0.1890392  & 0.01240303\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| parameter | logMSE | logMSESD |\n",
       "|---|---|---|\n",
       "| none       | 0.1890392  | 0.01240303 |\n",
       "\n"
      ],
      "text/plain": [
       "  parameter logMSE    logMSESD  \n",
       "1 none      0.1890392 0.01240303"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#BAGGING\n",
    "log.mse <- function (data,\n",
    "                        lev = NULL,\n",
    "                        model = NULL) {\n",
    "  out <-  sqrt(mean(   (log(data$obs) - log(abs(data$pred)))^2   ))\n",
    "  names(out) <- \"logMSE\"\n",
    "  out\n",
    "}\n",
    "\n",
    "bag.fit <- train(\n",
    "  SalePrice ~ ., \n",
    "    data = train, method = \"treebag\",\n",
    "  trControl = trainControl(\"cv\", number = 10 ,summaryFunction = log.mse ),\n",
    "  metric=\"logMSE\",\n",
    "    importance=TRUE,\n",
    "  tuneLength = 5\n",
    ")\n",
    "bag.fit$bestTune\n",
    "bag.fit$results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAAAAgP9NTU1oaGh8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\nljVBBsmyJsggWdYEGSTLmqD/D6Bak99NZjGrAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot( varImp(bag.fit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "\n",
      "Attaching package: 'randomForest'\n",
      "\n",
      "The following object is masked from 'package:dplyr':\n",
      "\n",
      "    combine\n",
      "\n",
      "The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    margin\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i  1  mtry  23  ntree  100 \n",
      "error is  0.1256776 \n",
      "\n",
      "i  2  mtry  88  ntree  50 \n",
      "error is  0.137677 \n",
      "\n",
      "i  3  mtry  24  ntree  200 \n",
      "error is  0.124375 \n",
      "\n",
      "i  4  mtry  33  ntree  150 \n",
      "error is  0.1248251 \n",
      "\n",
      "i  5  mtry  85  ntree  100 \n",
      "error is  0.1307201 \n",
      "\n",
      "i  6  mtry  79  ntree  50 \n",
      "error is  0.1307392 \n",
      "\n",
      "i  7  mtry  51  ntree  50 \n",
      "error is  0.1290375 \n",
      "\n",
      "i  8  mtry  78  ntree  150 \n",
      "error is  0.1323749 \n",
      "\n",
      "i  9  mtry  73  ntree  250 \n",
      "error is  0.1316358 \n",
      "\n",
      "i  10  mtry  89  ntree  250 \n",
      "error is  0.1329723 \n",
      "\n",
      "i  11  mtry  21  ntree  200 \n",
      "error is  0.1242867 \n",
      "\n",
      "i  12  mtry  70  ntree  50 \n",
      "error is  0.1323772 \n",
      "\n",
      "i  13  mtry  52  ntree  200 \n",
      "error is  0.1287881 \n",
      "\n",
      "i  14  mtry  31  ntree  100 \n",
      "error is  0.1258311 \n",
      "\n",
      "i  15  mtry  36  ntree  100 \n",
      "error is  0.1270773 \n",
      "\n",
      "i  16  mtry  42  ntree  200 \n",
      "error is  0.1273279 \n",
      "\n",
      "i  17  mtry  76  ntree  200 \n",
      "error is  0.1310097 \n",
      "\n",
      "i  18  mtry  82  ntree  150 \n",
      "error is  0.1310581 \n",
      "\n",
      "i  19  mtry  42  ntree  100 \n",
      "error is  0.1252672 \n",
      "\n",
      "i  20  mtry  34  ntree  250 \n",
      "error is  0.1256902 \n",
      "\n",
      "i  21  mtry  52  ntree  200 \n",
      "error is  0.1298602 \n",
      "\n",
      "i  22  mtry  37  ntree  250 \n",
      "error is  0.1255627 \n",
      "\n",
      "i  23  mtry  28  ntree  200 \n",
      "error is  0.1261229 \n",
      "\n",
      "i  24  mtry  45  ntree  200 \n",
      "error is  0.1271646 \n",
      "\n",
      "i  25  mtry  33  ntree  100 \n",
      "error is  0.1248689 \n",
      "\n",
      "i  26  mtry  56  ntree  250 \n",
      "error is  0.128633 \n",
      "\n",
      "i  27  mtry  62  ntree  250 \n",
      "error is  0.129779 \n",
      "\n",
      "i  28  mtry  75  ntree  200 \n",
      "error is  0.1305427 \n",
      "\n",
      "i  29  mtry  45  ntree  100 \n",
      "error is  0.1277965 \n",
      "\n",
      "i  30  mtry  76  ntree  150 \n",
      "error is  0.1284144 \n",
      "\n",
      "i  31  mtry  70  ntree  200 \n",
      "error is  0.1294101 \n",
      "\n",
      "i  32  mtry  82  ntree  250 \n",
      "error is  0.1315221 \n",
      "\n",
      "i  33  mtry  35  ntree  50 \n",
      "error is  0.1262243 \n",
      "\n",
      "i  34  mtry  37  ntree  50 \n",
      "error is  0.1240475 \n",
      "\n",
      "i  35  mtry  81  ntree  100 \n",
      "error is  0.1330203 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#HYPERTUNING THE RANDOM FOREST\n",
    "#random forests are a complex algorithm and hence suseptable to parameter selection, so I start with a random search\n",
    "library(randomForest)\n",
    "set.seed(4)\n",
    "params <- expand.grid(mtry=20:90, ntree=c(50,100,150,200,250))\n",
    "cv.length=35\n",
    "mtrys <- rep(0,cv.length)\n",
    "ntrees <- rep(0,cv.length)\n",
    "errors <- rep(0,cv.length)\n",
    "for (i in 1:cv.length) {\n",
    "    #select the parameters\n",
    "    par <- params[sample(nrow(params) , 1),]\n",
    "    mtrys[i] = par$mtry\n",
    "    ntrees[i] = par$ntree\n",
    "    cat(\"i \",i,\" mtry \", mtrys[i], \" ntree \", ntrees[i],\"\\n\")\n",
    "    \n",
    "    #train the model and calulate errors\n",
    "    rf.fit <- randomForest(SalePrice~., data=train.cv, mtry=mtrys[i] , ntree=ntrees[i])\n",
    "    errors[i]= sqrt(mean( (log( predict(rf.fit , newdata = cv)) - log(cv$SalePrice))^2 ))\n",
    "    cat(\"error is \", errors[i], \"\\n\")\n",
    "    cat(\"\\n\")\n",
    "    \n",
    "    #save the results to a csv incase training fails\n",
    "    rf.results <- data.frame(mtry = mtrys, ntree=ntrees, errors=errors)\n",
    "    write.csv(rf.results, file=\"Models/Trees/RandomForestsCVResults.csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i  1  mtry  30  ntree  250 \n",
      "error is  0.1234384 \n",
      "\n",
      "i  2  mtry  30  ntree  200 \n",
      "error is  0.1267525 \n",
      "\n",
      "i  3  mtry  18  ntree  250 \n",
      "error is  0.1238556 \n",
      "\n",
      "i  4  mtry  18  ntree  200 \n",
      "error is  0.1243797 \n",
      "\n",
      "i  5  mtry  18  ntree  250 \n",
      "error is  0.1232021 \n",
      "\n",
      "i  6  mtry  18  ntree  200 \n",
      "error is  0.1234874 \n",
      "\n",
      "i  7  mtry  25  ntree  200 \n",
      "error is  0.1255628 \n",
      "\n",
      "i  8  mtry  12  ntree  200 \n",
      "error is  0.1247534 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"Raster pixels are placed at uneven horizontal intervals and will be shifted. Consider using geom_tile() instead.\""
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAB5lBMVEUAAAAzMzNNTU1oaGh8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\ngJAg4PcBVHe1SkSGUWgAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAB6VBMVEUAAAAzMzNNTU1oaGh8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\nt0KG5NwK/X/MYKcMOK0uhAAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Looking at the results fewer mtry and more number of trees perform better\n",
    "ggplot(rf.results, aes(mtry, ntree, fill = errors)) + geom_raster()+ scale_fill_gradientn(colours=c(\"yellow\",\"red\"))\n",
    "\n",
    "#so I will perform another search for parameter values, but this time with a for focused search.\n",
    "\n",
    "#HYPERTUNING THE RANDOM FOREST\n",
    "#random forests are a complex algorithm and hence suseptable to parameter selection, so I start with a random search\n",
    "library(randomForest)\n",
    "set.seed(4)\n",
    "params <- expand.grid(mtry=c(12,18,25,30), ntree=c(200,250))\n",
    "cv.length=8\n",
    "mtrys <- rep(0,cv.length)\n",
    "ntrees <- rep(0,cv.length)\n",
    "errors <- rep(0,cv.length)\n",
    "for (i in 1:cv.length) {\n",
    "    #select the parameters\n",
    "    par <- params[sample(nrow(params) , 1),]\n",
    "    mtrys[i] = par$mtry\n",
    "    ntrees[i] = par$ntree\n",
    "    cat(\"i \",i,\" mtry \", mtrys[i], \" ntree \", ntrees[i],\"\\n\")\n",
    "    \n",
    "    #train the model and calulate errors\n",
    "    rf.fit <- randomForest(SalePrice~., data=train.cv, mtry=mtrys[i] , ntree=ntrees[i])\n",
    "    errors[i]= sqrt(mean( (log( predict(rf.fit , newdata = cv)) - log(cv$SalePrice))^2 ))\n",
    "    cat(\"error is \", errors[i], \"\\n\")\n",
    "    cat(\"\\n\")\n",
    "    \n",
    "    #save the results to a csv incase training fails\n",
    "    rf.results <- data.frame(mtry = mtrys, ntree=ntrees, errors=errors)\n",
    "    write.csv(rf.results, file=\"Models/Trees/RandomForestsCVResults_grid.csv\")\n",
    "}\n",
    "ggplot(rf.results, aes(mtry, ntree, fill = errors)) + geom_raster()+ scale_fill_gradientn(colours=c(\"yellow\",\"red\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.123309596979865"
      ],
      "text/latex": [
       "0.123309596979865"
      ],
      "text/markdown": [
       "0.123309596979865"
      ],
      "text/plain": [
       "[1] 0.1233096"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'call'</li>\n",
       "\t<li>'type'</li>\n",
       "\t<li>'predicted'</li>\n",
       "\t<li>'mse'</li>\n",
       "\t<li>'rsq'</li>\n",
       "\t<li>'oob.times'</li>\n",
       "\t<li>'importance'</li>\n",
       "\t<li>'importanceSD'</li>\n",
       "\t<li>'localImportance'</li>\n",
       "\t<li>'proximity'</li>\n",
       "\t<li>'ntree'</li>\n",
       "\t<li>'mtry'</li>\n",
       "\t<li>'forest'</li>\n",
       "\t<li>'coefs'</li>\n",
       "\t<li>'y'</li>\n",
       "\t<li>'test'</li>\n",
       "\t<li>'inbag'</li>\n",
       "\t<li>'terms'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'call'\n",
       "\\item 'type'\n",
       "\\item 'predicted'\n",
       "\\item 'mse'\n",
       "\\item 'rsq'\n",
       "\\item 'oob.times'\n",
       "\\item 'importance'\n",
       "\\item 'importanceSD'\n",
       "\\item 'localImportance'\n",
       "\\item 'proximity'\n",
       "\\item 'ntree'\n",
       "\\item 'mtry'\n",
       "\\item 'forest'\n",
       "\\item 'coefs'\n",
       "\\item 'y'\n",
       "\\item 'test'\n",
       "\\item 'inbag'\n",
       "\\item 'terms'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'call'\n",
       "2. 'type'\n",
       "3. 'predicted'\n",
       "4. 'mse'\n",
       "5. 'rsq'\n",
       "6. 'oob.times'\n",
       "7. 'importance'\n",
       "8. 'importanceSD'\n",
       "9. 'localImportance'\n",
       "10. 'proximity'\n",
       "11. 'ntree'\n",
       "12. 'mtry'\n",
       "13. 'forest'\n",
       "14. 'coefs'\n",
       "15. 'y'\n",
       "16. 'test'\n",
       "17. 'inbag'\n",
       "18. 'terms'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"call\"            \"type\"            \"predicted\"       \"mse\"            \n",
       " [5] \"rsq\"             \"oob.times\"       \"importance\"      \"importanceSD\"   \n",
       " [9] \"localImportance\" \"proximity\"       \"ntree\"           \"mtry\"           \n",
       "[13] \"forest\"          \"coefs\"           \"y\"               \"test\"           \n",
       "[17] \"inbag\"           \"terms\"          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Final RandomForest model\n",
    "library(randomForest)\n",
    "set.seed(42)\n",
    "rf.fit <- randomForest(SalePrice~., data=train.cv, mtry=20, ntree=400)\n",
    "#(log( predict(tree.fit , newdata = cv)) - log(cv$SalePrice))^2  %>% is.na() %>% any()\n",
    "sqrt(mean( (log( predict(rf.fit , newdata = cv)) - log(cv$SalePrice))^2 ))\n",
    "rf.fit %>% names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.predict <- data.frame(Id = test$Id, SalePrice = predict(rf.fit , newdata=test)) %>% arrange(Id)\n",
    "write.csv(test.predict, \"Models/Trees/Random Forest Predictions.csv\", row.names=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i  1  depth  5  ntree  300  shrinkage  0.1 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 4923161879.9965             nan     0.1000 707984775.7939\n",
      "     2 4317338327.2436             nan     0.1000 657445235.8000\n",
      "     3 3716847328.0478             nan     0.1000 556716636.5928\n",
      "     4 3265371046.0716             nan     0.1000 454135303.0180\n",
      "     5 2884601028.9710             nan     0.1000 361508491.7757\n",
      "     6 2584440564.7214             nan     0.1000 322385665.4748\n",
      "     7 2295473155.6332             nan     0.1000 303380169.1317\n",
      "     8 2063716381.9129             nan     0.1000 215573598.4104\n",
      "     9 1846951014.7723             nan     0.1000 193213748.8971\n",
      "    10 1673910570.6387             nan     0.1000 137845868.3520\n",
      "    20 754835757.0895             nan     0.1000 41575467.7338\n",
      "    40 383305184.5125             nan     0.1000 2320132.5149\n",
      "    60 295453647.7145             nan     0.1000 -3644533.9155\n",
      "    80 245649314.7245             nan     0.1000 -725417.6498\n",
      "   100 217875131.5123             nan     0.1000 76641.9394\n",
      "   120 196409877.7321             nan     0.1000 -682664.4115\n",
      "   140 176306776.1808             nan     0.1000 -700292.9805\n",
      "   160 162787527.7258             nan     0.1000 -1546640.5990\n",
      "   180 147686925.1012             nan     0.1000 -1472890.5142\n",
      "   200 136880716.1603             nan     0.1000 -1185625.9734\n",
      "   220 128167411.0113             nan     0.1000 -738530.0311\n",
      "   240 118265386.9666             nan     0.1000 -26223.8818\n",
      "   260 109469661.1855             nan     0.1000 -504769.8645\n",
      "   280 101125479.9528             nan     0.1000 -251934.0228\n",
      "   300 94943499.8356             nan     0.1000 -396761.4798\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 300 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1187074 \n",
      "\n",
      "i  2  depth  1  ntree  250  shrinkage  0.05 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5416243108.2463             nan     0.0500 271926038.1938\n",
      "     2 5154663295.5646             nan     0.0500 236542422.1153\n",
      "     3 4941278797.5157             nan     0.0500 211728621.0335\n",
      "     4 4733708053.6488             nan     0.0500 211235185.6029\n",
      "     5 4568396571.0222             nan     0.0500 167811877.5975\n",
      "     6 4387121129.5389             nan     0.0500 168695080.3784\n",
      "     7 4232114508.4550             nan     0.0500 152373435.8923\n",
      "     8 4066538086.8041             nan     0.0500 157262492.0529\n",
      "     9 3925996409.8908             nan     0.0500 135936392.0328\n",
      "    10 3796621549.3439             nan     0.0500 111362592.4037\n",
      "    20 2830105485.4591             nan     0.0500 70631309.5250\n",
      "    40 1782420163.3991             nan     0.0500 34069534.4369\n",
      "    60 1252294597.5361             nan     0.0500 16285864.1485\n",
      "    80 982811876.2147             nan     0.0500 2481742.3500\n",
      "   100 814667897.1867             nan     0.0500 4317239.9631\n",
      "   120 706113051.8418             nan     0.0500 84399.5237\n",
      "   140 632150412.4080             nan     0.0500 1379951.8442\n",
      "   160 582380880.8594             nan     0.0500 889085.9811\n",
      "   180 542225137.1318             nan     0.0500 960534.7285\n",
      "   200 512235327.1442             nan     0.0500 417069.1727\n",
      "   220 490220019.4011             nan     0.0500 325646.0136\n",
      "   240 472133183.9211             nan     0.0500 421371.2145\n",
      "   250 464782143.2423             nan     0.0500 213110.2650\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 250 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1401307 \n",
      "\n",
      "i  3  depth  2  ntree  200  shrinkage  0.5 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 3153248563.4312             nan     0.5000 2407143124.7536\n",
      "     2 2176599132.5036             nan     0.5000 1077213463.8049\n",
      "     3 1570413498.1597             nan     0.5000 548101542.9928\n",
      "     4 1281357585.8892             nan     0.5000 289345659.0275\n",
      "     5 1068381006.1741             nan     0.5000 147390541.8223\n",
      "     6 958153792.2214             nan     0.5000 78631950.3231\n",
      "     7 885422130.2063             nan     0.5000 58589399.2776\n",
      "     8 834251376.0734             nan     0.5000 15454464.2980\n",
      "     9 795630243.4156             nan     0.5000 25414282.7128\n",
      "    10 760007305.7022             nan     0.5000 3438476.4546\n",
      "    20 563531425.8132             nan     0.5000 10245214.1900\n",
      "    40 400139951.0554             nan     0.5000 -7520697.5212\n",
      "    60 340289152.4576             nan     0.5000 -7264196.7047\n",
      "    80 293499661.4087             nan     0.5000 -1762613.2314\n",
      "   100 261579152.5052             nan     0.5000 -2764417.1597\n",
      "   120 237590619.4014             nan     0.5000 -4343401.9534\n",
      "   140 217563004.3797             nan     0.5000 -1201881.6195\n",
      "   160 199591074.6716             nan     0.5000 -4318300.1956\n",
      "   180 188306066.6885             nan     0.5000 -4859602.8755\n",
      "   200 176444330.1626             nan     0.5000 -6993486.4701\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.133621 \n",
      "\n",
      "i  4  depth  2  ntree  200  shrinkage  0.05 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5346916931.7537             nan     0.0500 310082238.3076\n",
      "     2 5042142652.8487             nan     0.0500 290264587.2172\n",
      "     3 4760842930.7912             nan     0.0500 283167524.9291\n",
      "     4 4514804548.0053             nan     0.0500 248365743.4975\n",
      "     5 4285175373.1392             nan     0.0500 236773023.8027\n",
      "     6 4070281406.5751             nan     0.0500 225858468.0991\n",
      "     7 3872621309.3740             nan     0.0500 203372931.7853\n",
      "     8 3678657355.3815             nan     0.0500 179658344.2278\n",
      "     9 3497871215.3325             nan     0.0500 168244610.6222\n",
      "    10 3342575413.7805             nan     0.0500 152778571.5442\n",
      "    20 2227878741.8462             nan     0.0500 65938455.6051\n",
      "    40 1205444661.0276             nan     0.0500 27178320.4034\n",
      "    60 798670667.7945             nan     0.0500 14034086.8813\n",
      "    80 615095996.9167             nan     0.0500 4330792.0757\n",
      "   100 517957588.5146             nan     0.0500 2399434.9830\n",
      "   120 456872391.7803             nan     0.0500 391016.4842\n",
      "   140 414813619.3063             nan     0.0500 433859.7203\n",
      "   160 390331439.3159             nan     0.0500 568581.3020\n",
      "   180 371993431.8943             nan     0.0500 -166367.4448\n",
      "   200 355670078.2087             nan     0.0500 -897805.6036\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1280141 \n",
      "\n",
      "i  5  depth  4  ntree  100  shrinkage  1 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 1811812473.3816             nan     1.0000 3164244416.0857\n",
      "     2 1309129808.3293             nan     1.0000 405813228.0800\n",
      "     3 1155097609.2210             nan     1.0000 17597986.6745\n",
      "     4 1007936232.2125             nan     1.0000 28072456.6671\n",
      "     5 927259415.7862             nan     1.0000 29210539.7908\n",
      "     6 872400567.2534             nan     1.0000 25917383.9626\n",
      "     7 793788692.7618             nan     1.0000 36583095.3118\n",
      "     8 760877391.7945             nan     1.0000 -16109201.1518\n",
      "     9 734412714.3101             nan     1.0000 -20069486.4877\n",
      "    10 740449972.2903             nan     1.0000 -97404028.3464\n",
      "    20 620510892.6545             nan     1.0000 -39602135.6770\n",
      "    40 501262753.4029             nan     1.0000 -21269295.3307\n",
      "    60 425415102.1819             nan     1.0000 -38774102.0930\n",
      "    80 414540415.2896             nan     1.0000 -37870896.9391\n",
      "   100 344934740.1700             nan     1.0000 -7220892.5593\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 100 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.2731356 \n",
      "\n",
      "i  6  depth  5  ntree  100  shrinkage  0.5 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 2752403371.5883             nan     0.5000 2897057075.4983\n",
      "     2 1526566054.0081             nan     0.5000 1219182151.2681\n",
      "     3 1012053390.1318             nan     0.5000 373838808.0382\n",
      "     4 772415646.2409             nan     0.5000 190749375.2005\n",
      "     5 669428744.4518             nan     0.5000 72452723.2251\n",
      "     6 614078164.1958             nan     0.5000 18662350.7716\n",
      "     7 560916049.9882             nan     0.5000 7386440.1237\n",
      "     8 529863102.6622             nan     0.5000 8408878.6854\n",
      "     9 507724072.8968             nan     0.5000 -7318175.4174\n",
      "    10 482732941.6066             nan     0.5000 2348419.5898\n",
      "    20 360699794.4210             nan     0.5000 -9736014.7528\n",
      "    40 259426845.7900             nan     0.5000 -9083913.6173\n",
      "    60 194063421.5590             nan     0.5000 -6305350.8547\n",
      "    80 152595345.6860             nan     0.5000 -6932015.6757\n",
      "   100 123220414.4765             nan     0.5000 -6097688.3033\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 100 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.147257 \n",
      "\n",
      "i  7  depth  3  ntree  200  shrinkage  0.05 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5346310262.2420             nan     0.0500 354247579.7894\n",
      "     2 5019013805.8578             nan     0.0500 310979357.9679\n",
      "     3 4706192048.7739             nan     0.0500 291477362.6471\n",
      "     4 4447816260.4744             nan     0.0500 259629933.3384\n",
      "     5 4189919248.9344             nan     0.0500 232607928.6299\n",
      "     6 3942801584.3319             nan     0.0500 241660727.8440\n",
      "     7 3712007127.1476             nan     0.0500 216545194.4763\n",
      "     8 3512301589.0080             nan     0.0500 187720945.4818\n",
      "     9 3322371647.2523             nan     0.0500 159163327.8908\n",
      "    10 3144753410.5872             nan     0.0500 164514051.7884\n",
      "    20 1949821213.4347             nan     0.0500 76184798.4362\n",
      "    40 962041668.1583             nan     0.0500 21443964.7265\n",
      "    60 628554591.8386             nan     0.0500 7620340.5427\n",
      "    80 484169664.0311             nan     0.0500 3478894.8208\n",
      "   100 410584994.5363             nan     0.0500 2796604.0280\n",
      "   120 369103716.8995             nan     0.0500 1055236.5863\n",
      "   140 339510690.8733             nan     0.0500 -374996.0751\n",
      "   160 318977800.2290             nan     0.0500 -33591.1389\n",
      "   180 302763506.0969             nan     0.0500 -159683.0353\n",
      "   200 288173976.7289             nan     0.0500 -551552.4517\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1251676 \n",
      "\n",
      "i  8  depth  2  ntree  100  shrinkage  0.05 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5353110254.5992             nan     0.0500 331436438.5470\n",
      "     2 5047661946.1957             nan     0.0500 309745265.8659\n",
      "     3 4763392260.8783             nan     0.0500 274741816.8680\n",
      "     4 4515478497.8844             nan     0.0500 241481306.6971\n",
      "     5 4305548433.8471             nan     0.0500 223255482.1870\n",
      "     6 4091044570.6298             nan     0.0500 217778389.3816\n",
      "     7 3888082660.1095             nan     0.0500 187650137.1805\n",
      "     8 3722788374.6783             nan     0.0500 181821733.9003\n",
      "     9 3544374138.2836             nan     0.0500 161933806.7470\n",
      "    10 3381440832.1855             nan     0.0500 143087939.3366\n",
      "    20 2262549621.7613             nan     0.0500 75344680.1731\n",
      "    40 1233264338.2476             nan     0.0500 16215457.1109\n",
      "    60 830632243.5021             nan     0.0500 6078006.6477\n",
      "    80 635015288.2655             nan     0.0500 2514755.8692\n",
      "   100 530553095.3104             nan     0.0500 3827701.6497\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 100 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1504781 \n",
      "\n",
      "i  9  depth  3  ntree  150  shrinkage  0.5 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 2961736515.7445             nan     0.5000 2605661198.1818\n",
      "     2 1826251649.4852             nan     0.5000 1076222459.0753\n",
      "     3 1254723555.2398             nan     0.5000 499753772.0971\n",
      "     4 1005021427.7463             nan     0.5000 202000238.4588\n",
      "     5 836380731.1377             nan     0.5000 95378891.9631\n",
      "     6 775331902.0529             nan     0.5000 25408769.6398\n",
      "     7 736819475.5617             nan     0.5000 8148789.8501\n",
      "     8 696493061.2890             nan     0.5000 11117377.1059\n",
      "     9 669633027.2370             nan     0.5000 2227229.2399\n",
      "    10 632803013.9339             nan     0.5000 16903611.8310\n",
      "    20 460712973.5578             nan     0.5000 -6426311.2926\n",
      "    40 344957106.7297             nan     0.5000 -13125315.7513\n",
      "    60 283627468.5156             nan     0.5000 -9507603.5544\n",
      "    80 231901480.3096             nan     0.5000 -5402518.2858\n",
      "   100 201239112.8221             nan     0.5000 -8523701.5091\n",
      "   120 169136774.8078             nan     0.5000 -3545513.3247\n",
      "   140 148870105.2583             nan     0.5000 -3761907.1588\n",
      "   150 142806043.7515             nan     0.5000 -4326957.3458\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 150 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1491828 \n",
      "\n",
      "i  10  depth  3  ntree  300  shrinkage  0.05 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5321592658.9045             nan     0.0500 341240495.3827\n",
      "     2 4965048962.6352             nan     0.0500 331534650.8225\n",
      "     3 4667859003.0404             nan     0.0500 268144498.9520\n",
      "     4 4383738670.3688             nan     0.0500 258951580.8248\n",
      "     5 4145971397.2985             nan     0.0500 230421956.7527\n",
      "     6 3905967782.4155             nan     0.0500 229866199.7815\n",
      "     7 3686196813.3337             nan     0.0500 191958607.5181\n",
      "     8 3468571002.2874             nan     0.0500 220283814.3091\n",
      "     9 3284706507.5081             nan     0.0500 179431638.2379\n",
      "    10 3136168139.0190             nan     0.0500 157602658.9400\n",
      "    20 1945144934.8721             nan     0.0500 76627314.9996\n",
      "    40 975614492.3327             nan     0.0500 19022909.2627\n",
      "    60 628469041.2656             nan     0.0500 7914260.1265\n",
      "    80 478386116.9992             nan     0.0500 2252418.2038\n",
      "   100 405457325.7636             nan     0.0500 -95811.2498\n",
      "   120 362888402.8924             nan     0.0500 876792.6761\n",
      "   140 337551360.5353             nan     0.0500 -95371.2087\n",
      "   160 318007665.8711             nan     0.0500 50777.7175\n",
      "   180 301344160.6087             nan     0.0500 -287829.6643\n",
      "   200 285194077.8626             nan     0.0500 -683947.5348\n",
      "   220 272184566.9277             nan     0.0500 -560939.3750\n",
      "   240 262541164.3076             nan     0.0500 -1223353.2429\n",
      "   260 253359690.2451             nan     0.0500 27368.2551\n",
      "   280 244019228.4467             nan     0.0500 -444774.0522\n",
      "   300 235917336.8084             nan     0.0500 -417456.5146\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 300 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1211687 \n",
      "\n",
      "i  11  depth  5  ntree  300  shrinkage  0.5 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 2701161498.6766             nan     0.5000 2544262904.6132\n",
      "     2 1496206525.7857             nan     0.5000 1021988248.8503\n",
      "     3 1026536975.8446             nan     0.5000 360162422.2982\n",
      "     4 833362452.6990             nan     0.5000 153218656.9471\n",
      "     5 729726046.2896             nan     0.5000 50745046.6784\n",
      "     6 662386089.3334             nan     0.5000 29073193.8892\n",
      "     7 617243228.1357             nan     0.5000 3778365.4036\n",
      "     8 553361293.5979             nan     0.5000 21275290.2687\n",
      "     9 529113504.8269             nan     0.5000 -17774582.7993\n",
      "    10 514845466.5963             nan     0.5000 -19425190.7102\n",
      "    20 373047579.3559             nan     0.5000 -18225734.3277\n",
      "    40 260753845.6013             nan     0.5000 -10881062.2410\n",
      "    60 190053616.2921             nan     0.5000 -8654925.4888\n",
      "    80 150207720.1083             nan     0.5000 -5917257.2898\n",
      "   100 115543777.8918             nan     0.5000 -2575562.3603\n",
      "   120 96692222.1601             nan     0.5000 -5126394.9660\n",
      "   140 78331766.5667             nan     0.5000 -2614763.7696\n",
      "   160 63911195.9251             nan     0.5000 -967567.9751\n",
      "   180 54160009.3505             nan     0.5000 -1569558.7161\n",
      "   200 44316546.7335             nan     0.5000 -968392.5773\n",
      "   220 38694880.3402             nan     0.5000 -1145474.9341\n",
      "   240 32765095.6161             nan     0.5000 -1024683.3714\n",
      "   260 27825630.9066             nan     0.5000 -1153143.9395\n",
      "   280 24532859.6655             nan     0.5000 -1163139.1412\n",
      "   300 21116967.9735             nan     0.5000 -1267152.5179\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 300 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.165108 \n",
      "\n",
      "i  12  depth  3  ntree  250  shrinkage  0.001 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5681426490.8855             nan     0.0010 7467821.6473\n",
      "     2 5674073832.9726             nan     0.0010 6735943.1380\n",
      "     3 5667348058.1794             nan     0.0010 6298196.2740\n",
      "     4 5659879053.0521             nan     0.0010 7447082.3364\n",
      "     5 5652905618.7234             nan     0.0010 7270509.6575\n",
      "     6 5645726256.8678             nan     0.0010 7014582.1007\n",
      "     7 5639004409.9370             nan     0.0010 6243188.8392\n",
      "     8 5631677632.5617             nan     0.0010 6983213.7892\n",
      "     9 5624735857.5581             nan     0.0010 7290358.1535\n",
      "    10 5617448834.3230             nan     0.0010 7479375.9854\n",
      "    20 5546799194.3982             nan     0.0010 7124015.7562\n",
      "    40 5406858440.5217             nan     0.0010 7257195.2972\n",
      "    60 5271385662.8269             nan     0.0010 6142068.7328\n",
      "    80 5142090539.2998             nan     0.0010 5977751.6072\n",
      "   100 5015460973.9869             nan     0.0010 6442206.8348\n",
      "   120 4892731642.4601             nan     0.0010 6374921.1731\n",
      "   140 4773808205.0179             nan     0.0010 5457360.0089\n",
      "   160 4657573998.2624             nan     0.0010 5397258.0793\n",
      "   180 4547706736.7503             nan     0.0010 5683779.5526\n",
      "   200 4441830926.3856             nan     0.0010 5138133.8920\n",
      "   220 4338334978.9471             nan     0.0010 4270668.6513\n",
      "   240 4237394226.0754             nan     0.0010 4486507.6965\n",
      "   250 4188450929.5922             nan     0.0010 4437172.5205\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 250 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.3314533 \n",
      "\n",
      "i  13  depth  5  ntree  100  shrinkage  0.5 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 2655259596.0181             nan     0.5000 2885669478.5112\n",
      "     2 1546427947.7753             nan     0.5000 1031564671.9625\n",
      "     3 1064965010.0108             nan     0.5000 358659828.6535\n",
      "     4 861684067.8258             nan     0.5000 135703035.6126\n",
      "     5 742973083.7816             nan     0.5000 51121130.7429\n",
      "     6 678571277.4196             nan     0.5000 23852857.0824\n",
      "     7 624105222.0189             nan     0.5000 23697631.6461\n",
      "     8 583364397.9321             nan     0.5000 22853617.7241\n",
      "     9 548899396.5607             nan     0.5000 -10929569.2610\n",
      "    10 528635934.9139             nan     0.5000 -21489280.3871\n",
      "    20 383944405.8197             nan     0.5000 -14240422.5358\n",
      "    40 248181842.6773             nan     0.5000 -11871507.3046\n",
      "    60 186696196.4380             nan     0.5000 -5595639.2752\n",
      "    80 141573524.9221             nan     0.5000 -4168631.3175\n",
      "   100 112517591.0621             nan     0.5000 -4576635.2440\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 100 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1420628 \n",
      "\n",
      "i  14  depth  5  ntree  100  shrinkage  1 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 1697022990.0072             nan     1.0000 4399307383.1706\n",
      "     2 1239383852.1586             nan     1.0000 372618029.9627\n",
      "     3 1124153751.9259             nan     1.0000 -19034379.9208\n",
      "     4 1058379492.7113             nan     1.0000 -100845517.0895\n",
      "     5 998682998.2718             nan     1.0000 -59419038.9561\n",
      "     6 951720067.5610             nan     1.0000 -76539597.6896\n",
      "     7 910742876.4365             nan     1.0000 -61731939.7984\n",
      "     8 851685000.7865             nan     1.0000 -76441280.8675\n",
      "     9 835674448.6724             nan     1.0000 -102962522.1785\n",
      "    10 846257750.3844             nan     1.0000 -133896592.7250\n",
      "    20 733475872.2152             nan     1.0000 -54275696.7702\n",
      "    40 595459673.9610             nan     1.0000 -43648194.7273\n",
      "    60 495527081.8847             nan     1.0000 -9441013.6524\n",
      "    80 465433255.5284             nan     1.0000 -66877189.3312\n",
      "   100 402277122.6057             nan     1.0000 -41572115.1270\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 100 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.3693065 \n",
      "\n",
      "i  15  depth  3  ntree  300  shrinkage  0.05 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5342503140.8279             nan     0.0500 334900686.5715\n",
      "     2 5004214744.2719             nan     0.0500 306297163.4867\n",
      "     3 4685067385.0037             nan     0.0500 277781992.4576\n",
      "     4 4402136480.3861             nan     0.0500 262759771.6096\n",
      "     5 4140773857.2905             nan     0.0500 254559945.6015\n",
      "     6 3897526091.7055             nan     0.0500 242648675.7590\n",
      "     7 3684554524.2233             nan     0.0500 204768642.9832\n",
      "     8 3485740051.8812             nan     0.0500 196507968.9080\n",
      "     9 3311990572.4749             nan     0.0500 173881060.4943\n",
      "    10 3145436785.9423             nan     0.0500 164825138.0815\n",
      "    20 1970856471.2719             nan     0.0500 92503956.2000\n",
      "    40 992018789.8669             nan     0.0500 21903932.1255\n",
      "    60 643933034.1462             nan     0.0500 8168924.5966\n",
      "    80 493876915.7070             nan     0.0500 2838280.8119\n",
      "   100 414818830.5451             nan     0.0500 1278289.3064\n",
      "   120 372259251.2881             nan     0.0500 -1229572.7310\n",
      "   140 341555194.6764             nan     0.0500 372613.0637\n",
      "   160 320546773.0311             nan     0.0500 -84918.9762\n",
      "   180 303959139.0160             nan     0.0500 -1286.9154\n",
      "   200 288789976.3107             nan     0.0500 -448404.1093\n",
      "   220 275296078.0100             nan     0.0500 -505102.5483\n",
      "   240 263339889.7593             nan     0.0500 -761364.4388\n",
      "   260 252864321.2488             nan     0.0500 -758821.5157\n",
      "   280 244897317.3499             nan     0.0500 -842590.5465\n",
      "   300 235160075.6815             nan     0.0500 -622319.3721\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 300 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1171124 \n",
      "\n",
      "i  16  depth  2  ntree  300  shrinkage  0.01 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5620197042.4074             nan     0.0100 66580162.5998\n",
      "     2 5551578262.7824             nan     0.0100 64991431.6733\n",
      "     3 5484498829.9219             nan     0.0100 59483929.2142\n",
      "     4 5422447025.9967             nan     0.0100 59804744.9863\n",
      "     5 5361443454.1315             nan     0.0100 60358537.5615\n",
      "     6 5303093054.8821             nan     0.0100 52926948.5111\n",
      "     7 5243679507.4716             nan     0.0100 58290157.7946\n",
      "     8 5182576882.6321             nan     0.0100 58130497.7317\n",
      "     9 5125582987.2233             nan     0.0100 55928377.7289\n",
      "    10 5071419843.9277             nan     0.0100 57745115.7222\n",
      "    20 4541260820.9037             nan     0.0100 47854891.0019\n",
      "    40 3704641979.5926             nan     0.0100 36087607.2106\n",
      "    60 3091573921.1122             nan     0.0100 26563551.7418\n",
      "    80 2619997938.7551             nan     0.0100 19491063.3422\n",
      "   100 2241841503.3460             nan     0.0100 14543635.0782\n",
      "   120 1948410054.6126             nan     0.0100 14190317.5622\n",
      "   140 1702368924.5595             nan     0.0100 10232370.2576\n",
      "   160 1503306539.5334             nan     0.0100 7744214.1724\n",
      "   180 1342587079.3082             nan     0.0100 4886263.2225\n",
      "   200 1208431797.2054             nan     0.0100 2838772.1003\n",
      "   220 1098893268.2516             nan     0.0100 4612681.9111\n",
      "   240 1003369087.5927             nan     0.0100 3436425.8256\n",
      "   260 923524492.6460             nan     0.0100 3340757.3660\n",
      "   280 858653998.4868             nan     0.0100 2981573.5734\n",
      "   300 803393837.4899             nan     0.0100 1444246.3172\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 300 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.173994 \n",
      "\n",
      "i  17  depth  3  ntree  250  shrinkage  0.05 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5330327226.3537             nan     0.0500 343240674.4527\n",
      "     2 5025537879.7757             nan     0.0500 319736551.3970\n",
      "     3 4721677710.3176             nan     0.0500 282148024.1561\n",
      "     4 4427767630.5173             nan     0.0500 262713576.3119\n",
      "     5 4152599344.6248             nan     0.0500 236972696.7963\n",
      "     6 3920255323.3521             nan     0.0500 232911184.4965\n",
      "     7 3698388043.1928             nan     0.0500 225554645.8411\n",
      "     8 3506495711.9664             nan     0.0500 191664376.5966\n",
      "     9 3318989511.2328             nan     0.0500 192100666.8410\n",
      "    10 3142339830.4485             nan     0.0500 167619385.2833\n",
      "    20 1941519750.8026             nan     0.0500 79776140.0279\n",
      "    40 977627430.0392             nan     0.0500 22040189.8052\n",
      "    60 634565409.6571             nan     0.0500 8183153.9312\n",
      "    80 485268881.0550             nan     0.0500 3350326.9065\n",
      "   100 409845605.6152             nan     0.0500 668029.8244\n",
      "   120 368299623.0407             nan     0.0500 -1624373.6094\n",
      "   140 338807395.5413             nan     0.0500 389246.3800\n",
      "   160 316967926.0780             nan     0.0500 -1321275.7071\n",
      "   180 299758738.8190             nan     0.0500 -250998.7775\n",
      "   200 285243506.5222             nan     0.0500 -361436.5482\n",
      "   220 272761073.4322             nan     0.0500 -26187.9669\n",
      "   240 262663599.5921             nan     0.0500 -311364.6880\n",
      "   250 259447165.8857             nan     0.0500 -1304552.1975\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 250 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1239401 \n",
      "\n",
      "i  18  depth  5  ntree  100  shrinkage  0.001 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5681267564.6386             nan     0.0010 7197047.2777\n",
      "     2 5673049039.4619             nan     0.0010 7851032.8920\n",
      "     3 5664986105.6354             nan     0.0010 7720525.1268\n",
      "     4 5657403180.8380             nan     0.0010 7831077.4650\n",
      "     5 5649160507.8433             nan     0.0010 8512490.8168\n",
      "     6 5641540327.3146             nan     0.0010 7857426.1342\n",
      "     7 5633243886.7137             nan     0.0010 8231432.2395\n",
      "     8 5625325121.6714             nan     0.0010 7117531.3334\n",
      "     9 5617576504.4808             nan     0.0010 7619987.9186\n",
      "    10 5609846939.6130             nan     0.0010 7768510.6638\n",
      "    20 5531309719.0900             nan     0.0010 7774048.5860\n",
      "    40 5378429898.7869             nan     0.0010 7091970.8734\n",
      "    60 5230325675.5739             nan     0.0010 7659939.4520\n",
      "    80 5087687535.9931             nan     0.0010 6873279.4738\n",
      "   100 4950174640.1087             nan     0.0010 6272601.6372\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 100 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.3566191 \n",
      "\n",
      "i  19  depth  3  ntree  250  shrinkage  0.5 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 2840571120.8681             nan     0.5000 2493666298.4583\n",
      "     2 1729154254.9996             nan     0.5000 912989421.8622\n",
      "     3 1337859627.2031             nan     0.5000 368915919.1942\n",
      "     4 1097576708.0151             nan     0.5000 215297997.8931\n",
      "     5 942239494.7019             nan     0.5000 83206674.6904\n",
      "     6 849691425.7681             nan     0.5000 55938513.9404\n",
      "     7 794956966.6930             nan     0.5000 13363891.7253\n",
      "     8 727779089.6764             nan     0.5000 28269681.6409\n",
      "     9 694646438.8528             nan     0.5000 -3136449.9755\n",
      "    10 652231211.9783             nan     0.5000 19657695.1811\n",
      "    20 476820164.2375             nan     0.5000 8954630.2867\n",
      "    40 321377278.0303             nan     0.5000 -4838630.0007\n",
      "    60 267959260.2293             nan     0.5000 -13238974.1139\n",
      "    80 228095091.2499             nan     0.5000 -8221777.5152\n",
      "   100 201817822.6194             nan     0.5000 -4430410.4204\n",
      "   120 181162680.8545             nan     0.5000 -2412495.1050\n",
      "   140 158477990.5054             nan     0.5000 -3229705.3796\n",
      "   160 142479541.8551             nan     0.5000 -5005092.4072\n",
      "   180 130843587.5762             nan     0.5000 -1866136.9205\n",
      "   200 117943420.3342             nan     0.5000 -2880257.9549\n",
      "   220 102465145.6830             nan     0.5000 -1471991.0686\n",
      "   240 94809624.6583             nan     0.5000 -2571304.4758\n",
      "   250 90281048.6829             nan     0.5000 -2275220.5135\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 250 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1376474 \n",
      "\n",
      "i  20  depth  3  ntree  250  shrinkage  0.5 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 2942169498.6450             nan     0.5000 2459639587.5564\n",
      "     2 1831324480.1653             nan     0.5000 965635258.4001\n",
      "     3 1265970833.1133             nan     0.5000 496006626.6553\n",
      "     4 980718070.5136             nan     0.5000 288286165.0421\n",
      "     5 848878595.8943             nan     0.5000 90177037.4033\n",
      "     6 750051974.7489             nan     0.5000 63096301.7275\n",
      "     7 670355983.0528             nan     0.5000 45351439.8800\n",
      "     8 628498998.5828             nan     0.5000 10071560.7189\n",
      "     9 603638513.8845             nan     0.5000 2496075.9888\n",
      "    10 580192021.4495             nan     0.5000 2658240.4897\n",
      "    20 432113703.7729             nan     0.5000 -13382476.3942\n",
      "    40 325559753.6876             nan     0.5000 -9777029.2539\n",
      "    60 267927722.3798             nan     0.5000 -9707850.9353\n",
      "    80 224570802.0231             nan     0.5000 -6288496.4992\n",
      "   100 197117616.1276             nan     0.5000 -6688617.6481\n",
      "   120 171068834.6942             nan     0.5000 -2195300.4544\n",
      "   140 151827539.2260             nan     0.5000 -4348395.8070\n",
      "   160 134125723.4700             nan     0.5000 -3159863.4872\n",
      "   180 119173083.8384             nan     0.5000 -2465938.3472\n",
      "   200 107878504.9283             nan     0.5000 -2740519.4116\n",
      "   220 97551651.5425             nan     0.5000 -4213164.6914\n",
      "   240 89597112.0164             nan     0.5000 -3024377.9083\n",
      "   250 85703965.2324             nan     0.5000 -1705309.1685\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 250 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.147777 \n",
      "\n",
      "i  21  depth  4  ntree  150  shrinkage  0.05 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5300677103.5317             nan     0.0500 359568081.6175\n",
      "     2 4959490028.2516             nan     0.0500 331782216.8899\n",
      "     3 4634266088.1101             nan     0.0500 268760244.0815\n",
      "     4 4330884623.5167             nan     0.0500 287648300.4591\n",
      "     5 4067458572.8311             nan     0.0500 238417902.2905\n",
      "     6 3816513714.8826             nan     0.0500 230298813.9763\n",
      "     7 3591338886.3027             nan     0.0500 230631798.1822\n",
      "     8 3393341188.8037             nan     0.0500 191514447.6960\n",
      "     9 3188284689.5868             nan     0.0500 180830820.9083\n",
      "    10 2999280980.6194             nan     0.0500 182931674.5483\n",
      "    20 1787850326.0556             nan     0.0500 89736541.9701\n",
      "    40 843939923.9907             nan     0.0500 23754585.5839\n",
      "    60 534100878.8982             nan     0.0500 6110886.9481\n",
      "    80 410801790.8711             nan     0.0500 936730.4265\n",
      "   100 350850405.8004             nan     0.0500 696806.4904\n",
      "   120 314332497.6224             nan     0.0500 719825.2413\n",
      "   140 288251281.3548             nan     0.0500 -1080855.8713\n",
      "   150 276952688.9019             nan     0.0500 -795267.9747\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 150 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1235895 \n",
      "\n",
      "i  22  depth  4  ntree  300  shrinkage  0.01 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5609837866.3044             nan     0.0100 75194097.6597\n",
      "     2 5531962118.0125             nan     0.0100 74500598.9352\n",
      "     3 5458066270.2878             nan     0.0100 83606788.9600\n",
      "     4 5383158758.6908             nan     0.0100 70060264.7568\n",
      "     5 5308420743.2801             nan     0.0100 69378350.9828\n",
      "     6 5240388970.2811             nan     0.0100 65636785.5080\n",
      "     7 5172055259.8280             nan     0.0100 63212005.8458\n",
      "     8 5107334235.3406             nan     0.0100 61218442.7355\n",
      "     9 5040702531.3921             nan     0.0100 63295269.4933\n",
      "    10 4971427910.5012             nan     0.0100 72747959.8358\n",
      "    20 4363659735.6109             nan     0.0100 56155919.1487\n",
      "    40 3411410338.1735             nan     0.0100 39756513.2846\n",
      "    60 2723287579.1861             nan     0.0100 32075674.7271\n",
      "    80 2202459495.3355             nan     0.0100 20515512.9470\n",
      "   100 1817227969.1286             nan     0.0100 13983096.4724\n",
      "   120 1513162030.3922             nan     0.0100 10011497.0032\n",
      "   140 1278820529.1028             nan     0.0100 8685210.8651\n",
      "   160 1099535265.7589             nan     0.0100 5731589.5947\n",
      "   180 963026860.9958             nan     0.0100 5305002.2505\n",
      "   200 852462276.4755             nan     0.0100 4848452.4824\n",
      "   220 762517174.6083             nan     0.0100 3240143.8864\n",
      "   240 691533111.8709             nan     0.0100 1949729.2594\n",
      "   260 629286625.9916             nan     0.0100 2967325.3393\n",
      "   280 579740247.0495             nan     0.0100 1994289.6027\n",
      "   300 538358332.3656             nan     0.0100 1268375.3827\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 300 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.150526 \n",
      "\n",
      "i  23  depth  1  ntree  100  shrinkage  0.5 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 3721338706.1228             nan     0.5000 1940826306.2840\n",
      "     2 2712912992.1086             nan     0.5000 1073422954.4437\n",
      "     3 2217736566.3461             nan     0.5000 475305670.9905\n",
      "     4 1914889894.3640             nan     0.5000 158314231.3618\n",
      "     5 1622475527.8090             nan     0.5000 239671029.5871\n",
      "     6 1409988336.5002             nan     0.5000 200428747.8461\n",
      "     7 1277830895.3755             nan     0.5000 128780630.4117\n",
      "     8 1189627690.8805             nan     0.5000 81526337.0981\n",
      "     9 1127108890.7347             nan     0.5000 55183574.3010\n",
      "    10 1068869898.8905             nan     0.5000 26818604.7731\n",
      "    20 812590158.6170             nan     0.5000 -28088747.1250\n",
      "    40 604672578.2883             nan     0.5000 4467973.2526\n",
      "    60 524787666.2582             nan     0.5000 -56593.5819\n",
      "    80 491265684.5897             nan     0.5000 -9325854.6419\n",
      "   100 448882133.1470             nan     0.5000 -6291049.4813\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 100 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1467999 \n",
      "\n",
      "i  24  depth  5  ntree  250  shrinkage  0.01 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5605089693.2549             nan     0.0100 78444553.2611\n",
      "     2 5530578352.0283             nan     0.0100 78669176.4751\n",
      "     3 5449860067.7470             nan     0.0100 79219187.2264\n",
      "     4 5370353887.2811             nan     0.0100 72672486.7123\n",
      "     5 5294842543.6065             nan     0.0100 76653326.2207\n",
      "     6 5220430506.6385             nan     0.0100 67755616.5352\n",
      "     7 5151423075.8007             nan     0.0100 71635755.7875\n",
      "     8 5078760857.6813             nan     0.0100 71777752.4887\n",
      "     9 5007131428.3118             nan     0.0100 65829426.3501\n",
      "    10 4939955725.2557             nan     0.0100 67114496.8213\n",
      "    20 4312585245.8430             nan     0.0100 56133731.9469\n",
      "    40 3324972813.0204             nan     0.0100 37975249.1342\n",
      "    60 2613205374.8804             nan     0.0100 27242744.0067\n",
      "    80 2095546082.7155             nan     0.0100 21945236.6785\n",
      "   100 1708512476.2998             nan     0.0100 14625325.3577\n",
      "   120 1410230387.0800             nan     0.0100 10832651.8841\n",
      "   140 1180170843.7704             nan     0.0100 9278398.3302\n",
      "   160 1004814808.5145             nan     0.0100 6847877.4915\n",
      "   180 868638943.9833             nan     0.0100 4858147.3346\n",
      "   200 763461591.1497             nan     0.0100 3534034.2524\n",
      "   220 681747429.3031             nan     0.0100 2636291.3323\n",
      "   240 613495561.8832             nan     0.0100 2276730.5497\n",
      "   250 584998541.7066             nan     0.0100 1984902.6958\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 250 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1542253 \n",
      "\n",
      "i  25  depth  4  ntree  250  shrinkage  0.001 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5681507003.6666             nan     0.0010 6865944.5503\n",
      "     2 5673604224.8153             nan     0.0010 6586704.6796\n",
      "     3 5665860685.8225             nan     0.0010 7123561.1687\n",
      "     4 5657891511.4949             nan     0.0010 7190197.4986\n",
      "     5 5650198656.9031             nan     0.0010 7029625.5500\n",
      "     6 5642653289.8890             nan     0.0010 8105289.6821\n",
      "     7 5634865754.9732             nan     0.0010 6852792.0402\n",
      "     8 5627485803.4653             nan     0.0010 6997127.3790\n",
      "     9 5619451331.3899             nan     0.0010 8288020.2153\n",
      "    10 5611743722.5044             nan     0.0010 6973076.3169\n",
      "    20 5534976914.4852             nan     0.0010 7796466.3436\n",
      "    40 5386477076.8915             nan     0.0010 7210717.3829\n",
      "    60 5243581599.4856             nan     0.0010 6043765.1700\n",
      "    80 5105153653.9179             nan     0.0010 6763627.1719\n",
      "   100 4972334248.4214             nan     0.0010 6523990.9682\n",
      "   120 4842982194.7066             nan     0.0010 5894151.2024\n",
      "   140 4719579659.4803             nan     0.0010 5195367.0536\n",
      "   160 4597913605.4137             nan     0.0010 5030166.5499\n",
      "   180 4481538672.1844             nan     0.0010 5200229.4052\n",
      "   200 4368976686.3380             nan     0.0010 5324211.7758\n",
      "   220 4259768583.8736             nan     0.0010 4899852.0972\n",
      "   240 4154931901.2469             nan     0.0010 5219236.1707\n",
      "   250 4103041425.1450             nan     0.0010 5724929.0253\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 250 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.3268278 \n",
      "\n",
      "i  26  depth  2  ntree  200  shrinkage  1 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 2366638036.3801             nan     1.0000 3556059790.6126\n",
      "     2 1745393478.8563             nan     1.0000 477211174.7299\n",
      "     3 1462928785.1321             nan     1.0000 217727750.8294\n",
      "     4 1299778161.8260             nan     1.0000 107912220.5310\n",
      "     5 1207062401.8270             nan     1.0000 22308486.3602\n",
      "     6 1132524388.6034             nan     1.0000 2270979.9394\n",
      "     7 1091009013.8668             nan     1.0000 -35278589.0932\n",
      "     8 1000341509.1636             nan     1.0000 47043866.7423\n",
      "     9 942041490.9088             nan     1.0000 14395266.5867\n",
      "    10 920734299.2366             nan     1.0000 -32863689.8852\n",
      "    20 675939616.4573             nan     1.0000 -16479621.0512\n",
      "    40 563568991.1123             nan     1.0000 -15345825.7418\n",
      "    60 480755032.8202             nan     1.0000 -15151205.4266\n",
      "    80 442415236.1961             nan     1.0000 -22811622.2681\n",
      "   100 398682343.5274             nan     1.0000 -18950664.2859\n",
      "   120 349288851.1394             nan     1.0000 -20095561.0012\n",
      "   140 335981469.3487             nan     1.0000 -29833330.3226\n",
      "   160 319051928.5004             nan     1.0000 4291196.1316\n",
      "   180 306687517.4126             nan     1.0000 -11939992.5802\n",
      "   200 281263589.6956             nan     1.0000 -7668132.7000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 200 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1799919 \n",
      "\n",
      "i  27  depth  3  ntree  250  shrinkage  0.1 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 4947801711.5515             nan     0.1000 659112504.0906\n",
      "     2 4346387691.9314             nan     0.1000 549608156.6754\n",
      "     3 3876552650.7317             nan     0.1000 424474632.6851\n",
      "     4 3477759739.1692             nan     0.1000 372755241.7576\n",
      "     5 3127300307.1084             nan     0.1000 328982267.7061\n",
      "     6 2812508287.1417             nan     0.1000 304410435.8616\n",
      "     7 2578958360.7174             nan     0.1000 242710919.6146\n",
      "     8 2346466801.8491             nan     0.1000 207903542.5335\n",
      "     9 2137915282.5563             nan     0.1000 179636077.4208\n",
      "    10 1964321765.9465             nan     0.1000 165527975.8302\n",
      "    20 964185146.2258             nan     0.1000 52463126.8805\n",
      "    40 489568828.4266             nan     0.1000 6098681.3037\n",
      "    60 382765839.7746             nan     0.1000 -2600036.8941\n",
      "    80 336375547.9860             nan     0.1000 801209.0825\n",
      "   100 303344510.0924             nan     0.1000 -159264.6659\n",
      "   120 277892041.8547             nan     0.1000 -1416727.0200\n",
      "   140 257688574.0404             nan     0.1000 -368964.4084\n",
      "   160 239388417.9734             nan     0.1000 -685525.6093\n",
      "   180 225391336.5914             nan     0.1000 -2343874.0869\n",
      "   200 213776031.3695             nan     0.1000 -468072.7708\n",
      "   220 203237642.8428             nan     0.1000 -534701.3063\n",
      "   240 192952398.7587             nan     0.1000 -743705.5187\n",
      "   250 188489895.1061             nan     0.1000 -762908.4532\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 250 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.1213458 \n",
      "\n",
      "i  28  depth  4  ntree  100  shrinkage  0.01 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5611729014.8382             nan     0.0100 78071602.0835\n",
      "     2 5531413896.2046             nan     0.0100 77480565.0095\n",
      "     3 5457059246.5857             nan     0.0100 69522206.4208\n",
      "     4 5382906402.6741             nan     0.0100 70564933.7849\n",
      "     5 5313399412.5584             nan     0.0100 72628878.0004\n",
      "     6 5241487289.8941             nan     0.0100 76854264.7091\n",
      "     7 5170207986.2096             nan     0.0100 69076387.6801\n",
      "     8 5099265117.5662             nan     0.0100 65916054.0757\n",
      "     9 5034985430.3587             nan     0.0100 63600603.4978\n",
      "    10 4968510214.7928             nan     0.0100 66334274.7500\n",
      "    20 4357678869.1527             nan     0.0100 52109462.8331\n",
      "    40 3413234489.8968             nan     0.0100 35517357.9094\n",
      "    60 2722133164.7753             nan     0.0100 25771952.5904\n",
      "    80 2201900679.1435             nan     0.0100 21598236.8977\n",
      "   100 1810388246.3047             nan     0.0100 15027051.3596\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 100 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.2311429 \n",
      "\n",
      "i  29  depth  1  ntree  100  shrinkage  0.01 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5636373118.3850             nan     0.0100 51100438.7455\n",
      "     2 5588710074.9698             nan     0.0100 50575902.1781\n",
      "     3 5535902086.5792             nan     0.0100 50279677.9876\n",
      "     4 5488033725.9552             nan     0.0100 49564254.9253\n",
      "     5 5442393239.8370             nan     0.0100 49991578.6221\n",
      "     6 5391604542.5823             nan     0.0100 46936429.3250\n",
      "     7 5345327792.7611             nan     0.0100 47544667.2149\n",
      "     8 5296281393.7353             nan     0.0100 47259305.7749\n",
      "     9 5246544042.8070             nan     0.0100 45127831.7049\n",
      "    10 5200371666.8890             nan     0.0100 45990312.5063\n",
      "    20 4774603299.4426             nan     0.0100 36454449.7366\n",
      "    40 4106343155.8804             nan     0.0100 27813522.0633\n",
      "    60 3601800271.1323             nan     0.0100 20774316.0149\n",
      "    80 3198132074.6746             nan     0.0100 18312447.6246\n",
      "   100 2859676207.5544             nan     0.0100 15209615.6041\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 100 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.2773488 \n",
      "\n",
      "i  30  depth  4  ntree  300  shrinkage  0.001 \n",
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5681275760.3704             nan     0.0010 6986970.2012\n",
      "     2 5673404640.7162             nan     0.0010 7930231.0863\n",
      "     3 5665873309.7877             nan     0.0010 7303897.2418\n",
      "     4 5657948016.3828             nan     0.0010 7429263.6035\n",
      "     5 5650330248.7054             nan     0.0010 6965893.6588\n",
      "     6 5642855133.2633             nan     0.0010 6983726.6877\n",
      "     7 5635367450.8455             nan     0.0010 7600448.3037\n",
      "     8 5627712699.0249             nan     0.0010 8194216.2454\n",
      "     9 5619629013.1980             nan     0.0010 7727395.8028\n",
      "    10 5611872918.4309             nan     0.0010 7015420.4830\n",
      "    20 5538074472.3239             nan     0.0010 7392622.0292\n",
      "    40 5390451931.2972             nan     0.0010 7446951.4391\n",
      "    60 5246403384.8420             nan     0.0010 7116411.2275\n",
      "    80 5106922402.7235             nan     0.0010 6374835.5165\n",
      "   100 4971281946.0844             nan     0.0010 6498272.5307\n",
      "   120 4843243681.3587             nan     0.0010 6834311.5571\n",
      "   140 4718662431.6322             nan     0.0010 6164819.8132\n",
      "   160 4597440238.3258             nan     0.0010 5516050.0319\n",
      "   180 4480739777.7832             nan     0.0010 4878701.1771\n",
      "   200 4369000817.5973             nan     0.0010 5733219.2794\n",
      "   220 4259384066.2873             nan     0.0010 5053258.6771\n",
      "   240 4155055453.7869             nan     0.0010 4186627.9260\n",
      "   260 4052883079.4668             nan     0.0010 4159825.8828\n",
      "   280 3954115057.1529             nan     0.0010 4440590.2824\n",
      "   300 3858086023.9110             nan     0.0010 4433135.9617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 300 trees...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error is  0.3178483 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(gbm)\n",
    "#HYPERTUNING BOOSTING TREES\n",
    "#Similar to random forests I will start with a random search for hyper parameters\n",
    "set.seed(4)\n",
    "params <- expand.grid(depth=1:5,\n",
    "    ntree=c(100,150,200,250, 300),shrinkage=c(1,0.5,0.1,0.05,0.01,0.001) )\n",
    "cv.length=30\n",
    "depths <- rep(0,cv.length)\n",
    "ntrees <- rep(0,cv.length)\n",
    "shrinks <- rep(0,cv.length)\n",
    "errors <- rep(0,cv.length)\n",
    "for (i in 1:cv.length) {\n",
    "    #select the parameters\n",
    "    par <- params[sample(nrow(params) , 1),]\n",
    "    shrinks[i] = par$shrinkage\n",
    "    depths[i] = par$depth\n",
    "    ntrees[i] = par$ntree\n",
    "    cat(\"i \",i,\" depth \", depths[i], \" ntree \", ntrees[i],\" shrinkage \", shrinks[i],\"\\n\")\n",
    "    \n",
    "    #train the model and calulate errors\n",
    "    boost.fit <- gbm(SalePrice~., data=train.cv, distribution=\"gaussian\" , interaction.depth=depths[i], \n",
    "                 shrinkage=shrinks[i],n.trees=ntrees[i], verbose=TRUE)\n",
    "    errors[i]= sqrt(mean( log(abs(predict(boost.fit , newdata = cv)) - log(cv$SalePrice))^2 ))\n",
    "    \n",
    "    if(is.na(errors[i])){ stop(\"NANS produced\")}\n",
    "    \n",
    "    cat(\"error is \", errors[i], \"\\n\")\n",
    "    cat(\"\\n\")\n",
    "    \n",
    "    #save the results to a csv incase training fails\n",
    "    boost.results <- data.frame(Shrinkage = shrinks, ntree=ntrees, depths=depths, errors=errors)\n",
    "    write.csv(boost.results, file=\"Models/Trees/BoostingCVResults.csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5618114263.3985             nan     0.0100 72768219.9898\n",
      "     2 5549113084.8742             nan     0.0100 67526723.7399\n",
      "     3 5484785927.2870             nan     0.0100 68186502.1232\n",
      "     4 5421421352.9484             nan     0.0100 59137750.8777\n",
      "     5 5357213222.9173             nan     0.0100 62506802.1415\n",
      "     6 5293652895.5144             nan     0.0100 60301842.3633\n",
      "     7 5233496199.1667             nan     0.0100 57776008.4428\n",
      "     8 5176401174.7676             nan     0.0100 55326507.1727\n",
      "     9 5118206581.7807             nan     0.0100 61851129.9834\n",
      "    10 5058922948.0665             nan     0.0100 59856632.4882\n",
      "    20 4529098116.0519             nan     0.0100 46658526.9331\n",
      "    40 3703937402.9985             nan     0.0100 34803600.1341\n",
      "    60 3097615930.0962             nan     0.0100 24413781.6463\n",
      "    80 2633322092.3969             nan     0.0100 20979076.1197\n",
      "   100 2265661438.3494             nan     0.0100 17816137.6261\n",
      "   120 1963564515.1238             nan     0.0100 10789205.9321\n",
      "   140 1722478729.2823             nan     0.0100 8531431.9590\n",
      "   160 1530279832.7355             nan     0.0100 6819236.6561\n",
      "   180 1366068159.8075             nan     0.0100 7015686.9374\n",
      "   200 1230979695.4631             nan     0.0100 4880833.6472\n",
      "   220 1119541872.3925             nan     0.0100 5058728.6595\n",
      "   240 1024722747.7569             nan     0.0100 3564442.1991\n",
      "   260 943921640.3898             nan     0.0100 2317757.8686\n",
      "   280 877701134.3511             nan     0.0100 2457548.0550\n",
      "   300 818991194.7672             nan     0.0100 1818387.3380\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 300 trees...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.174897814783428"
      ],
      "text/latex": [
       "0.174897814783428"
      ],
      "text/markdown": [
       "0.174897814783428"
      ],
      "text/plain": [
       "[1] 0.1748978"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(gbm)\n",
    "set.seed(44)\n",
    "boost.fit <- gbm(SalePrice~., train.cv, distribution=\"gaussian\" , interaction.depth=2, shrinkage=0.01,\n",
    "                 n.tree = 300,verbose=TRUE)\n",
    "sqrt(mean( (log( predict(boost.fit , newdata = cv)) - log(cv$SalePrice))^2 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 5024334397.5620             nan     0.1000 626362489.4810\n",
      "     2 4457955306.4234             nan     0.1000 541729291.1160\n",
      "     3 4008534408.1350             nan     0.1000 449929958.2408\n",
      "     4 3632954767.8671             nan     0.1000 371773965.1993\n",
      "     5 3329716392.5357             nan     0.1000 330665499.5367\n",
      "     6 3035926529.1494             nan     0.1000 291809954.0679\n",
      "     7 2784884218.3710             nan     0.1000 206381375.2463\n",
      "     8 2557659519.4131             nan     0.1000 227726823.8060\n",
      "     9 2378731340.2298             nan     0.1000 172186569.7413\n",
      "    10 2211499581.4263             nan     0.1000 173896728.3262\n",
      "    20 1205021438.5473             nan     0.1000 55230064.3368\n",
      "    40 641689307.3049             nan     0.1000 10022417.4462\n",
      "    60 486817875.1172             nan     0.1000 -1081089.5309\n",
      "    80 422197619.5849             nan     0.1000 1608950.2306\n",
      "   100 386235978.8382             nan     0.1000 401369.1198\n",
      "   120 356377466.6350             nan     0.1000 -3389816.5445\n",
      "   140 339207092.6252             nan     0.1000 -973825.1868\n",
      "   160 319866408.1165             nan     0.1000 -354524.6762\n",
      "   180 303755674.9299             nan     0.1000 -808903.8161\n",
      "   200 288490991.9798             nan     0.1000 -672240.4376\n",
      "   220 274696036.2717             nan     0.1000 -1520337.0925\n",
      "   240 264087199.7096             nan     0.1000 -1027057.5622\n",
      "   260 255144725.3674             nan     0.1000 -467682.2479\n",
      "   280 245889278.1875             nan     0.1000 -597198.0271\n",
      "   300 237275199.9168             nan     0.1000 -571006.0803\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 300 trees...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.12103561127388"
      ],
      "text/latex": [
       "0.12103561127388"
      ],
      "text/markdown": [
       "0.12103561127388"
      ],
      "text/plain": [
       "[1] 0.1210356"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boost.fit <- gbm(SalePrice~., data=train.cv, distribution=\"gaussian\" , interaction.depth=2, \n",
    "                 shrinkage=0.1,n.trees=300, verbose=TRUE)\n",
    "sqrt(mean( (log(predict(boost.fit , newdata = cv)) - log(cv$SalePrice))^2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   TrainDeviance   ValidDeviance   StepSize   Improve\n",
      "     1 4996010143.4485             nan     0.1000 680270293.8937\n",
      "     2 4399968663.0596             nan     0.1000 592895430.2054\n",
      "     3 3891795340.1491             nan     0.1000 501255599.1168\n",
      "     4 3480746919.0043             nan     0.1000 412734828.4096\n",
      "     5 3122416594.4847             nan     0.1000 319256618.7272\n",
      "     6 2807974084.6452             nan     0.1000 346605609.2435\n",
      "     7 2548923655.8533             nan     0.1000 182253593.6982\n",
      "     8 2318231605.4372             nan     0.1000 201520144.8038\n",
      "     9 2110621152.5212             nan     0.1000 203059238.2713\n",
      "    10 1929464821.4625             nan     0.1000 169405452.4051\n",
      "    20 962738836.2235             nan     0.1000 47286500.8562\n",
      "    40 498472633.8576             nan     0.1000 4957013.3827\n",
      "    60 378639610.9811             nan     0.1000 32302.4176\n",
      "    80 330477051.0282             nan     0.1000 -327637.9900\n",
      "   100 300412088.3853             nan     0.1000 -2207721.9365\n",
      "   120 276308077.6446             nan     0.1000 -720290.7819\n",
      "   140 255460194.0519             nan     0.1000 -815150.4119\n",
      "   160 237715267.6647             nan     0.1000 -917577.9095\n",
      "   180 221180233.9509             nan     0.1000 -1991515.8850\n",
      "   200 209107053.5523             nan     0.1000 -1339001.1324\n",
      "   220 198233345.9851             nan     0.1000 -1159484.7064\n",
      "   240 187159757.1488             nan     0.1000 -633654.4276\n",
      "   260 176914301.6713             nan     0.1000 -740915.1027\n",
      "   280 168105099.9257             nan     0.1000 221729.1161\n",
      "   300 161394348.6405             nan     0.1000 -919964.8201\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 300 trees...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.117743401611508"
      ],
      "text/latex": [
       "0.117743401611508"
      ],
      "text/markdown": [
       "0.117743401611508"
      ],
      "text/plain": [
       "[1] 0.1177434"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boost.fit <- gbm(SalePrice~., data=train.cv, distribution=\"gaussian\" , interaction.depth=3, \n",
    "                 shrinkage=0.1,n.trees=300, verbose=TRUE)\n",
    "sqrt(mean( (log(predict(boost.fit , newdata = cv)) - log(cv$SalePrice))^2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 300 trees...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.predict <- data.frame(Id = test$Id, SalePrice = predict(boost.fit , newdata=test)) %>% arrange(Id)\n",
    "write.csv(test.predict, \"Models/Trees/Boosting Predictions.csv\", row.names=FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERTUNING THE XGBOOST\n",
    "#similar to randomforsts I will start with a random search\n",
    "library(xgboost)\n",
    "set.seed(5)\n",
    "params <- expand.grid(eta=seq(0.1,0.3,0.05) , gamma=c(0,0,1,0.2), max_depth=10:25, nround=30)\n",
    "cv.length=10\n",
    "etas <- rep(0,cv.length)\n",
    "depths <- rep(0,cv.length)\n",
    "gammas <- rep(0,cv.length)\n",
    "errors <- rep(0,cv.length)\n",
    "nrounds <- rep(0,cv.length)\n",
    "\n",
    "\n",
    "for (i in 1:cv.length) {\n",
    "    #select the parameters\n",
    "    par <- params[sample(nrow(params) , 1),]\n",
    "    depths[i] = par$max_depth\n",
    "    etas[i] = par$eta\n",
    "    gammas[i] = par$gamma\n",
    "    nrounds[i] = par$nround\n",
    "    cat(\"i \",i,\" eta \", etas[i], \" depth \", depths[i],\"gamma\", gammas[i],\"nrounds\",nrounds[i], \"\\n\")\n",
    "    \n",
    "    #train the model and calulate errors\n",
    "    xg.fit <- xgboost(data=train.cv.matrix,label=as.matrix(train.cv.matrix.labels),  eta = etas[i], max_depth = depths[i], \n",
    "                     gamma= gammas[i] , nrounds=nrounds[i])\n",
    "    errors[i]= sqrt(mean( (log( predict(xg.fit , newdata = cv.matrix)) - log(cv.matrix.labels))^2 ))\n",
    "    cat(\"error is \", errors[i], \"\\n\")\n",
    "    cat(\"\\n\")\n",
    "    \n",
    "    if(is.na(errors[i])){ stop(\"NANS produced\")}\n",
    "\n",
    "    #save the results to a csv incase training fails\n",
    "    rf.results <- data.frame(gamma = gammas, max_depth=depths, eta = etas, errors=errors)\n",
    "    write.csv(rf.results, file=\"Models/Trees/RandomForestsCVResults.csv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(depths, errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in xgb.train(params, dtrain, nrounds, watchlist, verbose = verbose, :\n",
      "\"xgb.train: `seed` is ignored in R package.  Use `set.seed()` instead.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:164725.328125 \n",
      "[2]\ttrain-rmse:139708.718750 \n",
      "[3]\ttrain-rmse:118622.960938 \n",
      "[4]\ttrain-rmse:100870.093750 \n",
      "[5]\ttrain-rmse:85913.812500 \n",
      "[6]\ttrain-rmse:73275.273438 \n",
      "[7]\ttrain-rmse:62633.085938 \n",
      "[8]\ttrain-rmse:53650.027344 \n",
      "[9]\ttrain-rmse:46069.511719 \n",
      "[10]\ttrain-rmse:39635.562500 \n",
      "[11]\ttrain-rmse:34174.800781 \n",
      "[12]\ttrain-rmse:29510.199219 \n",
      "[13]\ttrain-rmse:25545.220703 \n",
      "[14]\ttrain-rmse:22166.115234 \n",
      "[15]\ttrain-rmse:19257.882812 \n",
      "[16]\ttrain-rmse:16769.384766 \n",
      "[17]\ttrain-rmse:14606.125000 \n",
      "[18]\ttrain-rmse:12747.153320 \n",
      "[19]\ttrain-rmse:11139.407227 \n",
      "[20]\ttrain-rmse:9758.734375 \n",
      "[21]\ttrain-rmse:8562.008789 \n",
      "[22]\ttrain-rmse:7532.248047 \n",
      "[23]\ttrain-rmse:6640.013184 \n",
      "[24]\ttrain-rmse:5864.314941 \n",
      "[25]\ttrain-rmse:5166.481445 \n",
      "[26]\ttrain-rmse:4563.929199 \n",
      "[27]\ttrain-rmse:4040.347168 \n",
      "[28]\ttrain-rmse:3581.541992 \n",
      "[29]\ttrain-rmse:3188.184082 \n",
      "[30]\ttrain-rmse:2831.757812 \n"
     ]
    }
   ],
   "source": [
    "xg.fit <- xgboost(data=train.matrix,label=as.matrix(train$SalePrice),  eta = 0.21, max_depth = 20, \n",
    "                     gamma= 0.1 , seed=42, nrounds=30)\n",
    "#i  1  eta  0.16  depth  18 gamma 0 \n",
    "#i  3  eta  0.21  depth  20 gamma 0.1 \n",
    "\n",
    "test.predict <- data.frame(Id = test$Id, SalePrice = predict(xg.fit , newdata = test.matrix)) %>% arrange(Id)\n",
    "write.csv(test.predict, \"Models/Trees/XGBoost Predictions.csv\", row.names=FALSE)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
